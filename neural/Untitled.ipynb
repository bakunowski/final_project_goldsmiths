{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bakunowski/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "1.13.1\n",
      "2.2.4-tf\n",
      "Train dimensions (800, 45, 13)\n",
      "Train labels dimensions (800, 5)\n",
      "Test dimensions (200, 45, 13)\n",
      "Test labels dimensions (200, 5)\n",
      "WARNING:tensorflow:From /Users/bakunowski/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bakunowski/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 585)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 585)               342810    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               75008     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 418,463\n",
      "Trainable params: 418,463\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[ 0.8230839   0.01789357 -0.07594353 -0.44434726  0.8169522 ]\n",
      " [ 0.6294199   0.17124881  0.0303061  -0.00426348  0.4145323 ]\n",
      " [ 0.6439167   0.39445645  0.05976285 -0.08385779  0.40885895]\n",
      " [ 0.64799345  0.27393317 -0.13996139 -0.27097583  0.4045952 ]\n",
      " [ 0.5158637   0.3365328  -0.2418482   0.00369221  0.27899733]\n",
      " [ 0.6050019   0.37631577 -0.17044674 -0.05238901  0.41087213]\n",
      " [ 0.6913643   0.3505915  -0.20487165 -0.09390636  0.33724517]\n",
      " [ 0.54851264  0.23108298 -0.09713982  0.00628817  0.24213351]\n",
      " [ 0.600169    0.4059379  -0.23874196 -0.02754336  0.34869805]\n",
      " [ 0.38142347  0.26224595 -0.10292621  0.05512889  0.20870635]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import sys\n",
    "import tflearn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "with open(\n",
    "        '/Users/bakunowski/Documents/goldsmiths/2018_19/dissertation/project/neural/newest_data_1000_examples/MFCC.json'\n",
    ") as f:\n",
    "    d2 = json.load(f)\n",
    "\n",
    "mfcc = np.array(d2[\"lowlevel\"][\"mfcc\"])\n",
    "scaler.fit(mfcc)\n",
    "mfcc_norm = scaler.transform(mfcc)\n",
    "# normalized between 0 and 1 and reshaped to 45 frames x 13 values\n",
    "mfcc_norm_reshaped = mfcc_norm.reshape(1000, 45, 13)\n",
    "\n",
    "with open('/Users/bakunowski/Documents/goldsmiths/2018_19/dissertation/project/neural/newest_data_1000_examples/parameters.json') as f:\n",
    "    d1 = json.load(f)\n",
    "\n",
    "duration = np.array(d1[\"parameters\"][\"duration\"]).reshape(1000, 1)\n",
    "numberOfGrains = np.array(d1[\"parameters\"][\"numberOfGrains\"]).reshape(1000, 1)\n",
    "pitch = np.array(d1[\"parameters\"][\"pitch\"]).reshape(1000, 1)\n",
    "position = np.array(d1[\"parameters\"][\"position\"]).reshape(1000, 1)\n",
    "spread = np.array(d1[\"parameters\"][\"spread\"]).reshape(1000, 1)\n",
    "\n",
    "parameters = np.concatenate((duration, numberOfGrains, pitch, position, spread), axis=1)\n",
    "\n",
    "scaler.fit(parameters)\n",
    "# normalized between 0 and 1\n",
    "parameters_norm = scaler.transform(parameters)\n",
    "\n",
    "X_train = mfcc_norm_reshaped[:800]\n",
    "y_train = parameters_norm[:800]\n",
    "\n",
    "X_test = mfcc_norm_reshaped[800:1000]\n",
    "y_test = parameters_norm[800:1000]\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"Train dimensions\", X_train.shape)\n",
    "print(\"Train labels dimensions\", y_train.shape)\n",
    "print(\"Test dimensions\", X_test.shape)\n",
    "print(\"Test labels dimensions\", y_test.shape)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Flatten(input_shape=(45,13)),\n",
    "    layers.Dense(585, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(5)])\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mean_squared_error', 'accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "example_batch = X_train[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "print(example_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "13\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape[0])\n",
    "print(X_train[0].shape[1])\n",
    "print(y_train[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-52c70934f552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tflearn/models/dnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# TODO: check memory impact for large data and multiple optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         feed_dict = feed_dict_builder(X_inputs, Y_targets, self.inputs,\n\u001b[0;32m--> 184\u001b[0;31m                                       self.targets)\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mfeed_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mval_feed_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tflearn/utils.py\u001b[0m in \u001b[0;36mfeed_dict_builder\u001b[0;34m(X, Y, net_inputs, net_targets)\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;31m# If a dict is provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_iters = 40\n",
    "batch_size = 64\n",
    "number_hidden = 256\n",
    "parameter_size = y_train[0].shape[0]\n",
    "features_cols = X_train[0].shape[0]\n",
    "features_rows = X_train[0].shape[1]\n",
    "\n",
    "net = tflearn.input_data([None, 13, 45])\n",
    "net = tflearn.lstm(net, number_hidden, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, parameter_size, activation='relu')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=learning_rate,\n",
    "                         loss='mean_square')\n",
    "model = tflearn.DNN(net, tensorboard_verbose=3)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, n_epoch=training_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_test, y_test, batch_size=batch_size, n_epoch=training_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
