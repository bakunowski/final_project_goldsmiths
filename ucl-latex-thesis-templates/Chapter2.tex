\chapter{Literature Review}

\section{Problem background}

Mimicking sounds is very intuitive for humans. Humming along a song
we've heard previously, or generally repeating something so that it
sounds as close to the original as possible seems like a rather
trivial task for us. However, musicians who are able to translate what
they hear in their head, directly to an instrument, be it a physical
one like piano or guitar, or a software based instrument, are
considered viruosic.

Yet, many successful musicians might be programming synthesizers
following a not clearly defined `intuition', and often look for
inspiration for the sound they are trying to create.
% \cite{noauthor_oneohtrix_2016} \cite{herbert_manifesto_2011}
Especially today, a lot of sounds are created by the means of experimental
methods, often using presets as a starting point.

Therefore, to be able to replicate any sound in a synthesizer, based
only on an audio input would seem like a very intuitive way of
working, and interacting with synthesizers.

\subsection{Audio descriptors}

Unfortunately, based on how sound is represented digitally, mimicking
sounds is not as intuitive for computers as it is for humans. In order
to describe sounds we need some algorithms, that are capable of
generating meanigful data about given audio signal. A great amount of
these analysis tools exist, and generally they can be devided into two
categories. One dealing with timbre, and another dealing with rhythm,
or otherwise temoral qualities of the signal.

\subsubsection{Rhythm}

Onset detection, which tries to estimate how many `peaks' there are
in a signal is a very useful algorithm to estimate how much rhythmical
content there is present.

\subsubsection{Timbre}

In order to compute any sort of pitch detection, the audio signal has
to be translated from the time domain to the frequency domain. This
can be achieved by the inexplicably useful algorithm in music today: 
Fourier Transform. It can dissect a signal to it's most basic
sinusoidal components, therefore determining how much of which
frequencies are present in the signal, which in turn can directly
translate into pitch.

Building on that, possibly the most powerful algorithm for determining
the timbre of an audio signal is Mel-frequency cepstrum coefficients,
or MFCC. Many studies have been done to prove the usefullness of MFCCs
in describing the timbral, and as a matter of fact rythymical content
of audio signals. They are mainly used in speech symthesis and speech
recognition, however can be apply to any signal, and are a very
powerful descriptor.

\subsection{Granulation}

There is one, major limitation, when it comes to parameter predictions
for a granular, or any other corpus-based synthesizer. The sounds
created are heavily based on the sample fed into the synthesizer. Each
parameter changes meaning significantly, once the input sample is
changed. 

Two possible solutions come to mind, when trying to overcome this
problem. Using one input sample for synthesis, and trying
to predict samples for one perticular instance of the granulator is one.
Another, would be trying to fit some universal analysis algorithms,
that could decribe rhythym, pitch, density of sound etc. independently
of the original sample. This way any sample could be `molded' into
what would resemble the original prediction target. 

Concatenative synthesis is an alternative approach to this problem. It
could be beneficial, as it would try to find `grains' as closely
resembling parts of the target as possible, and recreate it using
little parts, almost like puzzles, that the algorithm thinks
fit. However, that approach also has limitations, as it could only
recreate the target sound out of the samples stored in it's database,
therefore making the output biased.

\subsection{Machine Learning (predictions?)}

A neural network could be used to build a parameter space for any
synthesizer from some input, which here would mean the audio analysis
algorithms results. A lot of research has been done on this
topic. From feedforward networks (cite) paired with analysis
algorithms, to Long Short Term Memory networks to take into account
the temporal information conveyed in different frames of the MFCC
analysis (citation). Convolutional networks are also very often used,
especially for classification of sounds.

Different algorithms were used previously to tackle the task of
automatic synthesizer programming, including Hill Climber (cite),
Genetic Algorithms (cite) etc.

Alternatively, in order to try and generalise the predictions for
different input samples for the synthesizer. The predictions could be
devided between different parameters. Then linking different
prediction algorithms with different parameters could possibly allow
for a creation of a more direct relationship between audio descriptors
and the synthesis results. 

%% ppr:
%Programming synthesizers is a fun, but challenging task. The range of sounds possible to
%achieve on such a device, and consequently the amount of adjustable parameters can be overwhelming.
%From choosing an audio file to sample, to an amplitude envelope for each grain, the ability to make
%decisions about programming a granulator has to come from either a place of certainty about what
%each parameter is responsible for, or a place of experimental thought, and a somewhat random
%parameter value assignments.

%Users fairly new in the realm of synthesizer programming may encounter issues creating sounds they
%desire. Even successful musicians might be doing things following a not clearly defined “intuition”.
%There are of course people who are experts in this field, but artists often look for inspiration
%when it comes to timbre of their sounds.
%%\cite{noauthor_oneohtrix_2016} \cite{herbert_manifesto_2011}
%It would seem that, the use of presets as a starting point is not unusual.

%My future thesis will aim to offer an alternative way to approach the creation of new sounds.

%%based on this write sections i think
%Research has been done previously as an investigation into automation of parameters in synthesizers
%based on sound matching. Taking a snippet of sound, the algorithm would try to
%find parameter settings to match a produced sound as closely as possible to the
%source%\cite{yee-king_automatic_2018}.

%This research mostly focuses on FM synthesis
%% \cite{horner_machine_1993}
%, although
%experiments on different synthesis techniques has been
%done
%% \cite{dahlstedt_creating_nodate}
%, including any VST
%plug-in
%% \cite{yee-king_synthbot:_nodate}.
%
%However, using this approach on corpus-based synthesis remains an untapped area
%of research, worth investigating
%%\cite{mcdonald_neural_2017}.
%
%Also, it seems that most of this research focuses on reproducing the original
%input
% \cite{tatar_automatic_2016}
%. Perhaps more interesting and novel sounds
%could arise as an effect of bad performance, but is does not seem to be the
%desired outcome in most cases.


%It seems like a description of this section is also in the introduction
%description. Which means that it could probably be fitted in there, instead of
%making a whole another section for it in the paper.

%% advice 

%The next paragraphs in the introduction should cite previous research in this
%area. It should cite those who had the idea or ideas first, and should also cite
%those who have done the most recent and relevant work. You should then go on to
%explain why more work was necessary (your work, of course.)

%Sufficient background information to allow the reader to understand the context
%and significance of the question you are trying to address.

%Proper acknowledgement of the previous work on which you are building.
%Sufficient references such that a reader could, by going to the library, achieve
%a sophisticated understanding of the context and significance of the question.

%The introduction should be focused on the thesis question(s). All cited work
%should be directly relevant to the goals of the thesis. This is not a place to
%summarize everything you have ever read on a subject.