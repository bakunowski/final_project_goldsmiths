\chapter{Literature Review}
\label{chapterlabel2}

\section{Problem background}
%VERY CONCISE 
%- what is the problem

Programming synthesizers is a fun, but challenging task. The range of sounds possible to
achieve on such a device, and consequently the amount of adjustable parameters can be overwhelming.
From choosing an audio file to sample, to an amplitude envelope for each grain, the ability to make
decisions about programming a granulator has to come from either a place of certainty about what
each parameter is responsible for, or a place of experimental thought, and a somewhat random
parameter value assignments.

Users fairly new in the realm of synthesizer programming may encounter issues creating sounds they
desire. Even successful musicians might be doing things following a not clearly defined “intuition”.
There are of course people who are experts in this field, but artists often look for inspiration
when it comes to timbre of their sounds.
%\cite{noauthor_oneohtrix_2016} \cite{herbert_manifesto_2011}
It would seem that, the use of presets as a starting point is not unusual.

%- some study that describes it directly, and is a general thing done on the main
%problem here - programming synths based on sound matching
%based on this write sections i think
Research has been done previously as an investigation into automation of parameters in synthesizers
based on sound matching. Taking a snippet of sound, the algorithm would try to
find parameter settings to match a produced sound as closely as possible to the
source%\cite{yee-king_automatic_2018}.

This research mostly focuses on FM synthesis
% \cite{horner_machine_1993}
, although
experiments on different synthesis techniques has been
done
% \cite{dahlstedt_creating_nodate}
, including any VST
plug-in
% \cite{yee-king_synthbot:_nodate}.

Also, it seems that most of this research focuses on reproducing the original
input
% \cite{tatar_automatic_2016}
. Perhaps more interesting and novel sounds
could arise as an effect of bad performance, but is does not seem to be the
desired outcome in most cases.

However, using this approach on corpus-based synthesis remains an untapped area
of research, worth investigating
%\cite{mcdonald_neural_2017}.

\subsection{Audio descriptors}

MFCCs are a very popular descriptor used widely in the domain of speech
synthesis. Some literature also uses it as the means of comparing two sounds
together(cite matthew, or some other study). It is probably the most
sophisticated algorithm for describing timbre available currently.

There is of course a huge variety of audio descriptors available, and some
serve different purpose than others. One can get very specific and use
descriptors for onsets, pitch, etc.

%what other can i mention that are useful, and widely used, but not as popular?
%mention the ones that i think will be useful in this project - paired with a granulator.
These really gain power when used in parallel, or together, rather than on their
own. That is the main reason behind the approach of describing audio in this
project.

Especially when trying to describe audio to a machine - an agent of you will,
that will be responsible for the programming of the granulator. Thinking about
it in terms of how humans would approach it, one would need to describe specific
characteristics of a particular sound to replicate it. Often this happens
'behind the scenes', as our perception does most of the work for us, and we
never really explicitly think about all the separate trains of information that
we have to process in order to achieve an understanding of sound.

In this specific case, we are approaching the programming of a granular
synthesizer, therefore we need specific metrics of describing sound that can
translate well to the parameters of a granular synthesizer.

\subsection{Granulation}

Automating parameters for granular synthesizer can be a difficult problem,
considering how heavily the output sound depends on the audio being sampled. In
other words the source of the grains.

To accommodate for that difficulty, some universal audio descriptors have to be
used in order to describe the sound to the previously mentioned agent. These
include (change when done) descriptions of rhythm, density, pitch etc, so that
the input sound can be 'molded' into what will resemble the original sound that
the agent is trying to replicate.

Concatenative synthesis - another thing widely used in speech synthesis offers a
solution to this problem by creating a 'pool' of grains to choose from in order
to replicate a sound (offer better description). It could be an interesting way
to approach this problem, however a lot of research has been done in this domain
(CataRT) and it seemed less experimental to tinker with. (the truth, but
obviously change later)

\subsection{Machine Learning}

A sophisticated neural network could be implemented to essentialy build a
granular synthesizer parameter space form the ground up using only the input
sound, however implementing such a system and predicting it's behaviour is a
difficult task as well as a very experimental approach, that is quite difficult
to control and predict.

Focusing on replicating certain aspects of the input sound seems like a more
straightforward and most importantly predictable approach to this problem.

Investigating the correlation between certain audio descriptors and their
influence over specific parameters of the synthesizer, and consequently
separating the machine learning tasks to predict those aspects of the sound
separately will result in a much more predictable outcome, as well as one that's
easier to control.
