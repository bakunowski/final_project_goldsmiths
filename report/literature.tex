\chapter{Literature Review}
\label{chap:lit}

\section{Problem background}

\subsection{Synthesizer programming}

Programming synthesizers can be enjoyable, fun and gratifying, yet
very often presents a big challenge. The amount of adjustable
parameters on such a device can be overwhelming, and consequently the
range of sounds that can be achieved is usually very extensive.

Granulation, or any other corpus based synthesis technique for that
matter, presents what could be called a special case in this
domain. Not only do the parameters determine the final output, but
also the input sample has a tremendous influence over the sound. In a
granular synthesizer, users are presented with choices ranging from
determining the audio to be sampled, to the amliptude envelope for
each grain. A huge array of possibilities.

Therefore the ability to make decisions about programming a granulator
to achieve desired results has to come from either a place of
certainty about what each parameter is responsible for combined with
intuition about the instrument, or a place of experimental though, and
somewhat random parameter value assignments.

Users fairly new in the realm of synthesizer programming may encounter
issues creating sounds they desire. This is to be expected, as like in
any other area, novices have to get through a rather steep learning
curve, to achieve certain intuition, and skill. This, however, often
prevents people from creating what they want, or makes them settle for
less, consequently limiting their creative output.

On the other hand, experienced musicians are constantly looking for
inspiration and new ways of interaction with the process of creating
exciting sounds\cite{herbert_manifesto_2011}. Furthermore, programming
synthesizers by following a not clearly defined intuition, combined
with the use of presets and a mixture of experimantal methods is a
popular approach\cite{noauthor_oneohtrix_2016}, especially considering
the amount of tools available today, and the ease of acquiring
them. Having too many VST plugins in a DAW is a common problem today.

There are of course all the people in between the complete novice, and
succesfull artist. They may have some experience, and can follow their
own intuition, yet are trying to find their own sound, and define
their style of production. Making that process easier, consequently
making the music making process more accessible would be a great
contribution to the community (EDIT, BUT IDEA IS GOOD).

\subsection{Sound matching}

The problem of sound matching seems to be almost unexistent for
humans. We are able to hum along a song we have heard previously, or
replicate sounds of different objects, like cars, or animals, like
dogs. This activity does not present any real challenge for us. It
could even be said that such behaviour is taken for
granted.

It has been around since forever, and it is mostly how children learn
language. The word ``onomatopoeia'' desribes a process of creating a
word that phonetically resembles, or suggests the sound it describes
(SOURCE) and has been around even before language came aobut (CHANGE)
(ADD SOURCE)!

Conversly, creating a specific desired sound on an instrument of any
kind takes serious practice, and sacrifices. Musicians who are able to
translate what they hear in their head, directly to an instrument, be
it a physical one like piano or guitar, or a software based
instrument, are considered viruosic. Therefore, the ability to
replicate any sound in a synthesizer, based solely on an audio input
seems like a very intuitive way of interaction, and possibly an
inspiring way of working for musicians. Additionally, it would blur the
line between a begginer, and a virtuoso in that specific area,
allowing people with less technical expertise to create music they
want to hear. Primarily, however, such interaction would be an immensly
helpful, and time-saving scenario for musicians, allowing them to
spend more time on creative work, taking over, at least partially the
task of programming a synthesiser. An attempt of solving this problem is undertaken in this papaer.

In the next three sections I go over each area of the project, and talk
about previous work that has been done on each aspect needed for such an interaction to be possible.

\section{Audio descriptors}

Because of the way audio is represented digitally, mimicking sounds is
not as intuitive for computers as it is for humans. Algorithms that
are capable of generating meaningful data about given audio signal are
needed in order to describe sounds in different ways, and based on
different assumptions. (add more about why these are needed, and pure
audio singal can't be used?)

A great amount of these analysis tools exist, many of which are
appropriate for musical applications\cite{peeters_timbre_2011}. A lot
of them are easily accessible either through libraries in different
programming languages, such as
`libriosa'\cite{noauthor_librosa_nodate} for Python, or
`Essentia'\cite{noauthor_homepage_nodate} for C++, or directly through
different audio software such as
`Audacity'\cite{noauthor_audacity_nodate} or most Digital Audio
Workstations today.

The early work done with a focus on sound matching was using mainly
spectral features\cite{mitchell_frequency_2005}, such as short time
Fourier analysis\cite{horner_machine_1993}. More recently, however,
the Mel-Frequency Cepstrum Coefficients seem to be the prefferable
descriptor\cite{davis_comparison_1980}\cite{yee-king_comparison_2011}.

\subsection{Mel-frequency Cepstrum Coefficients}

A more detailed explenation of the algorithm is provided in the
``Methods'' section in \autoref{chap:meth}, and only an attempt to
confirm their validity for musical applications is made below.

The majority of work wih MFCC have been directed at the problem of
speech recognition and synthesis. However, an impressive amount of
work has been done to prove the usefullness of the algorithm in
istrument recognition and Music Information Retrieval (MIR) systems.

Logan provides a credible justification for the use of the MFCC
in musical modelling, supporting the use of the warped frequency scale
and the Discrete Cosine
Transform\cite{noauthor_logan00mel.pdf_nodate}. Casey et al. present
an overview of the state-of-the-art content based MIR; the use of MFCC
is promoted for the purposes of timbral
description\cite{casey_content-based_2008}. Eronen shows that the MFCC
is the best single feature in a comparison of features for monophonic
instrument recognition tasks\cite{eronen_comparison_2001}. Brown et
al. present a study where cepstral coefficient prove to be the most
effective for woodwind instrument recognition
tasks\cite{brown_feature_2001}.

Additionally, MFCC vectors have been shown to be an effective audio feature for
sound matching
tasks\cite{yee-king_synthbot:_nodate}\cite{heise_automated_2009}

What is more, Terasawa et al. determined that a timbre space based on
MFCC is a good model for perceptual timbre
space\cite{terasawa_center_2005}.(more !)

% try and refrase this:
% Listener studies have shown that
% movement in MFCC space is associated with a similar `sized' movement
% in human perceived timbre space\cite{terasawa_center_2005} (change as this sentence is a total ripoff)

\section{Synthesis}

%more about this:
Early research done as an investigation into automation of parameters
in synthesizers based on sound matching focused mainly on Frequency
Modulation synthesis\cite{horner_machine_1993}, although more recently
experiments on different synthesis techniques have been
done\cite{dahlstedt_creating_nodate}\cite{yee-king_comparison_2011},
including a wide variety of VST
plug-ins\cite{yee-king_synthbot:_nodate}.

Yee-King et al. have shown that the more advanced synthesizer engine,
the more difficult it is to predict the parameters, despitie prediction
algorithms used\cite{yee-king_automatic_2018}. The different
algorithms will be disscussed in the next section.

% all the examples above were wavetable synthesis (or whatever) not corpus-based
However, this applies purely to the amount of parameters, and not
neccessairly the type of synthesis used. Using this approach on
corpus-based synthesis remains an untapped area of research, worth
investigating\cite{mcdonald_neural_2017}.

%Taking a
%snippet of sound, the algorithm would try to find parameter settings
%to match a produced sound as closely as possible to the
%source\cite{yee-king_automatic_2018}.

\section{Predictions}

One of the first approaches to consider automatic synthesizer
programming was described by Horner et al.\cite{horner_machine_1993}
 and used a Genetic
Algorithm (GA) aimed at predicting settings for an FM
synthesizer. GA was also used in many systems
after\cite{dahlstedt_creating_nodate}, and remains a relevant
optimasation technique for this task.

Yee-King investigated different optimisation techniques, including
Hill Climbing, Genetic Algorithms, Neural Networks and data driven
approaches\cite{yee-king_automatic_nodate}. Showing that the Hill
Climber algorithm, and data driven approaches offer strong performance
in this task\cite{yee-king_comparison_2011}.

% add info about how long they can take maybe
The search based algorithms, such as Hill Climber or a Genetic
Algorithm offer slow performace, and require much computetional power
to come up with a result. Therefore, they do not seem to offer the
ability of prediction in real, or near real time, making them an
unappealing choice for user facing systems.

% say how long is a neural net trained for ?
On the other hand, a modeling approach such as a neural network, offer
near-real time performance once trained. The training process can
dependt on many factors, such as the size of a dataset, and number of
parameter to predict\cite{yee-king_automatic_nodate}, but the ability
of real time prediction, makes them a much more valid approach for
user facing systems.

A more in depth exploartion of possibilities in this domain was done
by Yee-King et al. who compared and considered GA, HC, MLP, LSTM, and
LSTM++ algorithms\cite{yee-king_automatic_2018}.

%stick to this shit over here:
It seems that most of this research focuses on reproducing the
original input\cite{tatar_automatic_2016}. Perhaps more interesting
and novel sounds could arise as an effect of bad performance, but is
does not seem to be the desired outcome in most of the literature.

\section{Alternatives to sound matching}

%only expand if too much room
Another way of automating synthesizer parameters is a certain
remapping to a 3D visual space...

%% advice 

%The next paragraphs in the introduction should cite previous research in this
%area. It should cite those who had the idea or ideas first, and should also cite
%those who have done the most recent and relevant work. You should then go on to
%explain why more work was necessary (your work, of course.)

%Sufficient background information to allow the reader to understand the context
%and significance of the question you are trying to address.

%Proper acknowledgement of the previous work on which you are building.
%Sufficient references such that a reader could, by going to the library, achieve
%a sophisticated understanding of the context and significance of the question.

%The introduction should be focused on the thesis question(s). All cited work
%should be directly relevant to the goals of the thesis. This is not a place to
%summarize everything you have ever read on a subject.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dissertation"
%%% End:
