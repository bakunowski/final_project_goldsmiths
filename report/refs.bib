
@article{yee-king_automatic_nodate,
	title = {Automatic {Sound} {Synthesizer} {Programming}: {Techniques} and {Applications}},
	abstract = {The aim of this thesis is to investigate techniques for, and applications of automatic sound synthesizer programming. An automatic sound synthesizer programmer is a system which removes the requirement to explicitly specify parameter settings for a sound synthesis algorithm from the user. Two forms of these systems are discussed in this thesis: tone matching programmers and synthesis space explorers. A tone matching programmer takes at its input a sound synthesis algorithm and a desired target sound. At its output it produces a conﬁguration for the sound synthesis algorithm which causes it to emit a similar sound to the target. The techniques for achieving this that are investigated are genetic algorithms, neural networks, hill climbers and data driven approaches. A synthesis space explorer provides a user with a representation of the space of possible sounds that a synthesizer can produce and allows them to interactively explore this space. The applications of automatic sound synthesizer programming that are investigated include studio tools, an autonomous musical agent and a self-reprogramming drum machine. The research employs several methodologies: the development of novel software frameworks and tools, the examination of existing software at the source code and performance levels and user trials of the tools and software. The main contributions made are: a method for visualisation of sound synthesis space and low dimensional control of sound synthesizers; a general purpose framework for the deployment and testing of sound synthesis and optimisation algorithms in the SuperCollider language sclang; a comparison of a variety of optimisation techniques for sound synthesizer programming; an analysis of sound synthesizer error surfaces; a general purpose sound synthesizer programmer compatible with industry standard tools; an automatic improviser which passes a loose equivalent of the Turing test for Jazz musicians, i.e. being half of a man-machine duet which was rated as one of the best sessions of 2009 on the BBC’s ’Jazz on 3’ programme.},
	language = {en},
	author = {Yee-King, Matthew John},
	pages = {180}
}

@article{yee-king_synthbot:_nodate,
	title = {{SYNTHBOT}: {AN} {UNSUPERVISED} {SOFTWARE} {SYNTHESIZER} {PROGRAMMER}},
	abstract = {This work presents a software synthesizer programmer, SynthBot, which is able to automatically ﬁnd the settings necessary to produce a sound similar to a given target. As modern synthesizers become more capable and the underlying synthesis architectures more obscure, the task of programming them to produce a desired sound becomes more time consuming and complex. SynthBot is presented as an automated solution to this problem. A stochastic search algorithm, in this case a genetic algorithm, is used to ﬁnd the parameters which produce the most similar sound to the target. Similarity is measured by the sum squared error between the Mel Frequency Cepstrum Coefﬁcients (MFCCs) of the target and candidate sounds.},
	language = {en},
	author = {Yee-King, Matthew and Roth, Martin},
	pages = {4}
}

@article{johnson_exploring_nodate,
	title = {Exploring the sound-space of synthesis algorithms using interactive genetic algorithms.},
	abstract = {Exploring the sounds available from a synthesis algorithm is a complicated process, requiring the user either to spend much time gaining heuristic experience with the algorithm or requiring them to have a deep knowledge of the underlying synthesis algorithms. In this paper we describe a computer system which facilitates a more exploratory approach to sound design, allowing the user to work at the level of the sounds themselves. In this system the synthesis parameters are managed by a genetic algorithm, which is directed by the users’ judgements about the sounds in the system. We describe the development of a prototype version of this system, concentrating on an interface to the FOF granular synthesis algorithm from CSound.},
	language = {en},
	author = {Johnson, Colin G},
	pages = {9}
}

@article{yee-king_automatic_2018,
	title = {Automatic {Programming} of {VST} {Sound} {Synthesizers} {Using} {Deep} {Networks} and {Other} {Techniques}},
	volume = {2},
	issn = {2471-285X},
	doi = {10.1109/TETCI.2017.2783885},
	abstract = {Programming sound synthesizers is a complex and time-consuming task. Automatic synthesizer programming involves finding parameters for sound synthesizers using algorithmic methods. Sound matching is one application of automatic programming, where the aim is to find the parameters for a synthesizer that cause it to emit as close a sound as possible to a target sound. We describe and compare several sound matching techniques that can be used to automatically program the Dexed synthesizer, which is a virtual model of a Yamaha DX7. The techniques are a hill climber, a genetic algorithm, and three deep neural networks that have not been applied to the problem before. We define a sound matching task based on six sets of sounds, which we derived from increasingly complex configurations of the Dexed synthesis algorithm. A bidirectional, long short-term memory network with highway layers performed better than any other technique and was able to match sounds closely in 25\% of the test cases. This network was also able to match sounds in near real time, once trained, which provides a significant speed advantage over previously reported techniques that are based on search heuristics. We also describe our open source framework, which makes it possible to repeat our study, and to adapt it to different synthesizers and algorithmic programming techniques.},
	number = {2},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	author = {Yee-King, M. J. and Fedden, L. and d'Inverno, M.},
	month = apr,
	year = {2018},
	keywords = {algorithmic methods, algorithmic programming, artificial neural networks, automatic programming, automatic synthesizer programming, Computer generated music, deep networks, deep neural networks, Dexed synthesis algorithm, Dexed synthesizer, electronic music, Feature extraction, Frequency modulation, frequency modulation (FM), genetic algorithms, Genetic algorithms, genetic algorithms (GA), music, neural nets, programming, Programming profession, short-term memory network, signal synthesis, sound matching, sound matching task, Synthesizers, target sound, Task analysis, VST sound synthesizers},
	pages = {150--159}
}

@book{noauthor_oneohtrix_2016,
	title = {Oneohtrix {Point} {Never} on the process behind his latest leap into the unknown, {Garden} of {Delete}},
	url = {https://www.musicradar.com/news/tech/oneohtrix-point-never-on-the-process-behind-his-latest-leap-into-the-unknown-garden-of-delete-633280},
	language = {EN\_GB},
	urldate = {2018-11-18},
	month = feb,
	year = {2016}
}

@book{herbert_manifesto_2011,
	title = {manifesto {\textbackslash}textbar {Matthew} {Herbert}},
	url = {https://matthewherbert.com/about-contact/manifesto/},
	language = {en-US},
	urldate = {2018-11-18},
	author = {Herbert, Matthew},
	year = {2011}
}

@book{mcdonald_neural_2017,
	title = {Neural {Nets} for {Generating} {Music}},
	url = {https://medium.com/artists-and-machine-intelligence/neural-nets-for-generating-music-f46dffac21c0},
	abstract = {Algorithmic music composition has developed a lot in the last few years, but the idea has a long history. In some sense, the first…},
	urldate = {2018-11-18},
	author = {McDonald, Kyle},
	month = aug,
	year = {2017}
}

@article{tatar_automatic_2016,
	title = {Automatic {Synthesizer} {Preset} {Generation} with \textit{{PresetGen}}},
	volume = {45},
	issn = {0929-8215, 1744-5027},
	url = {http://www.tandfonline.com/doi/full/10.1080/09298215.2016.1175481},
	doi = {10.1080/09298215.2016.1175481},
	abstract = {We refer the task of ﬁnding preset(s) (i.e. set(s) of synthesizer parameters) that approximates a target sound best, as the preset generation problem. PresetGen addresses this problem regarding the real world synthesizer, OP-1. The OP-1 consists of several synthesis blocks, and it is not fully deterministic. We propose and evaluate a solution to preset generation using a multi-objective Non-dominated Sorting-Genetic-AlgorithmII. PresetGen handles the full problem complexity and returns a small set of presets that approximate the target sound best by covering the Pareto front of this multi-objective optimization problem. Moreover, we present an empirical evaluation experiment that compares the performance of three human sound designers to that of PresetGen. The results show that PresetGen is human-competitive.},
	language = {en},
	number = {2},
	urldate = {2018-11-19},
	journal = {Journal of New Music Research},
	author = {Tatar, Kıvanç and Macret, Matthieu and Pasquier, Philippe},
	month = apr,
	year = {2016},
	pages = {124--144}
}

@book{noauthor_granulator_nodate,
	title = {Granulator by {Robert} {Henke}},
	url = {http://roberthenke.com/technology/granulator.html},
	urldate = {2018-11-19}
}

@article{gerhard_pitch_nodate,
	title = {Pitch {Extraction} and {Fundamental} {Frequency}: {History} and {Current} {Techniques}},
	abstract = {Pitch extraction (also called fundamental frequency estimation) has been a popular topic in many ﬁelds of research since the age of computers. Yet in the course of some 50 years of study, current techniques are still not to a desired level of accuracy and robustness. When presented with a single clean pitched signal, most techniques do well, but when the signal is noisy, or when there are multiple pitch streams, many current pitch algorithms still fail to perform well. This report presents a discussion of the history of pitch detection techniques, as well as a survey of the current state of the art in pitch detection technology.},
	language = {en},
	author = {Gerhard, David},
	pages = {23}
}

@book{muller_information_2007,
	title = {Information {Retrieval} for {Music} and {Motion}},
	isbn = {978-3-540-74048-3},
	abstract = {A general scenario that has attracted a lot of attention for multimedia information retrieval is based on the query-by-example paradigm: retrieve all documents from a database containing parts or aspects similar to a given data fragment. However, multimedia objects, even though they are similar from a structural or semantic viewpoint, often reveal significant spatial or temporal differences. This makes content-based multimedia retrieval a challenging research field with many unsolved problems. Meinard Müller details concepts and algorithms for robust and efficient information retrieval by means of two different types of multimedia data: waveform-based music data and human motion data. In Part I, he discusses in depth several approaches in music information retrieval, in particular general strategies as well as efficient algorithms for music synchronization, audio matching, and audio structure analysis. He also shows how the analysis results can be used in an advanced audio player to facilitate additional retrieval and browsing functionality. In Part II, he introduces a general and unified framework for motion analysis, retrieval, and classification, highlighting the design of suitable features, the notion of similarity used to compare data streams, and data organization. The detailed chapters at the beginning of each part give consideration to the interdisciplinary character of this field, covering information science, digital signal processing, audio engineering, musicology, and computer graphics. This first monograph specializing in music and motion retrieval appeals to a wide audience, from students at the graduate level and lecturers to scientists working in the above mentioned fields in academia or industry. Lecturers and students will benefit from the didactic style, and each unit is suitable for stand-alone use in specialized graduate courses. Researchers will be interested in the detailed description of original research results and their application in real-world browsing and retrieval scenarios.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Müller, Meinard},
	month = sep,
	year = {2007},
	keywords = {Computers / Computer Graphics, Computers / Computer Science, Computers / Data Processing, Computers / Databases / General, Computers / Desktop Applications / Design \& Graphics, Computers / General, Computers / Information Technology, Computers / Interactive \& Multimedia, Computers / System Administration / Storage \& Retrieval}
}

@book{noauthor_librosa_nodate,
	title = {Librosa},
	url = {https://librosa.github.io/},
	urldate = {2019-05-03}
}

@book{noauthor_homepage_nodate,
	title = {Homepage — {Essentia} 2.1-beta5-dev documentation},
	url = {https://essentia.upf.edu/documentation/},
	urldate = {2019-05-03}
}

@book{noauthor_audacity_nodate,
	title = {Audacity ® {\textbackslash}textbar {Free}, open source, cross-platform audio software for multi-track recording and editing.},
	url = {https://www.audacityteam.org/},
	language = {en-US},
	urldate = {2019-05-03}
}

@article{horner_machine_1993,
	title = {Machine {Tongues} {XVI}: {Genetic} {Algorithms} and {Their} {Application} to {FM} {Matching} {Synthesis}},
	volume = {17},
	issn = {01489267},
	shorttitle = {Machine {Tongues} {XVI}},
	url = {https://www.jstor.org/stable/3680541?origin=crossref},
	doi = {10.2307/3680541},
	language = {en},
	number = {4},
	urldate = {2019-05-03},
	journal = {Computer Music Journal},
	author = {Horner, Andrew and Beauchamp, James and Haken, Lippold},
	year = {1993},
	pages = {17}
}

@article{heise_automated_2009,
	title = {Automated {Cloning} of {Recorded} {Sounds} by {Software} {Synthesizers}},
	abstract = {Any audio recording can be turned into a digital musical instrument by feeding it into an audio sampler. However, it is difficult to edit such a sound in musical terms or even to control it in real time with musical expression. This does not change much if a more sophisticated resynthesis method is applied. Many electronic musicians appreciate the direct and clear access to sound parameters a traditional analog synthesizer offers. Can one automatically generate a synthesizer setting that approximates a given audio recording and thus clone a given sound to be controlled with the standard functions of the particular synthesizer employed? Even though this problem seems highly complex, we demonstrate that its solution becomes feasible with today’s computer systems. We compare sounds on the basis of acoustic features known from Music Information Retrieval and apply a specialized optimization strategy to adjust the settings of VST instruments, which is sped up using multi-core processors and networked computers.},
	language = {en},
	journal = {New York},
	author = {Heise, Sebastian and Hlatky, Michael and Loviscach, Jörn},
	year = {2009},
	pages = {8}
}

@inproceedings{eronen_comparison_2001,
	address = {New Platz, NY, USA},
	title = {Comparison of features for musical instrument recognition},
	isbn = {978-0-7803-7126-2},
	url = {http://ieeexplore.ieee.org/document/969532/},
	doi = {10.1109/ASPAA.2001.969532},
	abstract = {Several features were compared with regard to recognition performance in a musical instrument recognition system. Both melfrequency and linear prediction cepstral and delta cepstral coefﬁcients were calculated. Linear prediction analysis was carried out both on a uniform and a warped frequency scale, and reﬂection coefﬁcients were also used as features. The performance of earlier described features relating to the temporal development, modulation properties, brightness, and spectral synchronity of sounds was also analysed. The data base consisted of 5286 acoustic and synthetic solo tones from 29 different Western orchestral instruments, out of which 16 instruments were included in the test set. The best performance for solo tone recognition, 35\% for individual instruments and 77\% for families, was obtained with a feature set consisting of two sets of mel-frequency cepstral coefﬁcients and a subset of the other analysed features. The confusions made by the system were analysed and compared to results reported in a human perception experiment.},
	language = {en},
	urldate = {2019-05-03},
	booktitle = {Proceedings of the 2001 {IEEE} {Workshop} on the {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({Cat}. {No}.01TH8575)},
	publisher = {IEEE},
	author = {Eronen, A.},
	year = {2001},
	pages = {19--22}
}

@article{terasawa_center_2005,
	title = {Center for {Computer} {Research} in {Music} and {Acoustics} ({CCRMA}) {Department} of {Music}, {Stanford} {University} {Stanford}, {California}},
	abstract = {This paper describes a perceptual space for timbre, deﬁnes an objective metric that takes into account perceptual orthogonality, and measures the quality of timbre interpolation applicable to perceptually valid timbral soniﬁcation. We discuss two timbre representations and measure perceptual judgment. We determined that a timbre space based on Mel-frequency cepstral coefﬁcients (MFCC) is a good model for perceptual timbre space.},
	language = {en},
	author = {Terasawa, Hiroko and Slaney, Malcolm and Berger, Jonathan},
	year = {2005},
	pages = {8}
}

@article{mitchell_frequency_2005,
	title = {Frequency {Modulation} {Tone} {Matching} {Using} a {Fuzzy} {Clustering} {Evolution} {Strategy}},
	abstract = {Frequency Modulation parameter estimation has provided a continual challenge to researchers since its ﬁrst application to audio synthesis over thirty years ago. Recent research has made use of basic evolutionary optimisation algorithms to evolve sounds produced by non-standard Frequency Modulation arrangements. In contrast, this paper utilises recent advances in multi-modal evolutionary optimisation to perform dynamicsound matching with traditional arrangements. In doing so, a technique is developed that is not synthesiser dependent, and provides the potential for alternative methods of synthesis control.},
	language = {en},
	author = {Mitchell, Thomas J and Sullivan, J Charles W},
	year = {2005},
	pages = {12},
	file = {Mitchell and Sullivan - 2005 - Frequency Modulation Tone Matching Using a Fuzzy C.pdf:/home/bakunowski/Zotero/storage/29ENIH82/Mitchell and Sullivan - 2005 - Frequency Modulation Tone Matching Using a Fuzzy C.pdf:application/pdf}
}

@article{yee-king_comparison_2011,
	title = {A {Comparison} of {Parametric} {Optimisation} {Techniques} for {Musical} {Instrument} {Tone} {Matching}},
	abstract = {Parametric optimisation techniques are compared in their abilities to elicit parameter settings for sound synthesis algorithms which cause them to emit sounds as similar as possible to target sounds. A hill climber, a genetic algorithm, a neural net and a data driven approach are compared. The error metric used is the Euclidean distance in MFCC feature space. This metric is justiﬁed on the basis of its success in previous work. The genetic algorithm oﬀers the best results with the FM and subtractive test synthesizers but the hill climber and data driven approach also oﬀer strong performance. The concept of sound synthesis error surfaces, allowing the detailed description of sound synthesis space, is introduced. The error surface for an FM synthesizer is described and suggestions are made as to the resolution required to eﬀectively represent these surfaces. This information is used to inform future plans for algorithm improvements.},
	language = {en},
	author = {Yee-King, Matthew and Roth, Martin},
	year = {2011},
	pages = {9},
	file = {Yee-King and Roth - 2011 - A Comparison of Parametric Optimisation Techniques.pdf:/home/bakunowski/Zotero/storage/H2SARUPZ/Yee-King and Roth - 2011 - A Comparison of Parametric Optimisation Techniques.pdf:application/pdf}
}

@article{peeters_timbre_2011,
	title = {The {Timbre} {Toolbox}: {Extracting} audio descriptors from musical signals},
	volume = {130},
	issn = {0001-4966},
	shorttitle = {The {Timbre} {Toolbox}},
	url = {http://asa.scitation.org/doi/10.1121/1.3642604},
	doi = {10.1121/1.3642604},
	language = {en},
	number = {5},
	urldate = {2019-05-11},
	journal = {The Journal of the Acoustical Society of America},
	author = {Peeters, Geoffroy and Giordano, Bruno L. and Susini, Patrick and Misdariis, Nicolas and McAdams, Stephen},
	month = nov,
	year = {2011},
	pages = {2902--2916},
	file = {Peeters et al. - 2011 - The Timbre Toolbox Extracting audio descriptors f.pdf:/home/bakunowski/Zotero/storage/5QJ2JGJG/Peeters et al. - 2011 - The Timbre Toolbox Extracting audio descriptors f.pdf:application/pdf}
}

@article{davis_comparison_1980,
	title = {Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences},
	volume = {28},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1980.1163420},
	abstract = {Several parametric representations of the acoustic signal were compared with regard to word recognition performance in a syllable-oriented continuous speech recognition system. The vocabulary included many phonetically similar monosyllabic words, therefore the emphasis was on the ability to retain phonetically significant acoustic information in the face of syntactic and duration variations. For each parameter set (based on a mel-frequency cepstrum, a linear frequency cepstrum, a linear prediction cepstrum, a linear prediction spectrum, or a set of reflection coefficients), word templates were generated using an efficient dynamic warping method, and test data were time registered with the templates. A set of ten mel-frequency cepstrum coefficients computed every 6.4 ms resulted in the best performance, namely 96.5 percent and 95.0 percent recognition with each of two speakers. The superior performance of the mel-frequency cepstrum coefficients may be attributed to the fact that they better represent the perceptually relevant aspects of the short-term speech spectrum.},
	number = {4},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Davis, S. and Mermelstein, P.},
	month = aug,
	year = {1980},
	keywords = {Acoustic measurements, Acoustic testing, Band pass filters, Cepstrum, Filtering, Laboratories, Loudspeakers, Nonlinear filters, Speech analysis, Speech recognition},
	pages = {357--366},
	file = {IEEE Xplore Abstract Record:/home/bakunowski/Zotero/storage/5QMTPHY3/1163420.html:text/html;IEEE Xplore Full Text PDF:/home/bakunowski/Zotero/storage/LJPZ5G4A/Davis and Mermelstein - 1980 - Comparison of parametric representations for monos.pdf:application/pdf}
}

@misc{noauthor_logan00mel.pdf_nodate,
	title = {logan00mel.pdf},
	url = {http://musicweb.ucsd.edu/~sdubnov/CATbox/Reader/logan00mel.pdf},
	urldate = {2019-05-11},
	file = {logan00mel.pdf:/home/bakunowski/Zotero/storage/JWEN9SWJ/logan00mel.pdf:application/pdf}
}

@article{casey_content-based_2008,
	title = {Content-{Based} {Music} {Information} {Retrieval}: {Current} {Directions} and {Future} {Challenges}},
	volume = {96},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Content-{Based} {Music} {Information} {Retrieval}},
	url = {http://ieeexplore.ieee.org/document/4472077/},
	doi = {10.1109/JPROC.2008.916370},
	language = {en},
	number = {4},
	urldate = {2019-05-11},
	journal = {Proceedings of the IEEE},
	author = {Casey, M.A. and Veltkamp, R. and Goto, M. and Leman, M. and Rhodes, C. and Slaney, M.},
	month = apr,
	year = {2008},
	pages = {668--696},
	file = {Casey et al. - 2008 - Content-Based Music Information Retrieval Current.pdf:/home/bakunowski/Zotero/storage/R4YWHJ3A/Casey et al. - 2008 - Content-Based Music Information Retrieval Current.pdf:application/pdf}
}

@article{brown_feature_2001,
	title = {Feature dependence in the automatic identification of musical woodwind instruments},
	volume = {109},
	issn = {0001-4966},
	url = {http://scitation.aip.org/content/asa/journal/jasa/109/3/10.1121/1.1342075},
	doi = {10.1121/1.1342075},
	language = {en},
	number = {3},
	urldate = {2019-05-11},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brown, Judith C. and Houix, Olivier and McAdams, Stephen},
	month = mar,
	year = {2001},
	pages = {1064--1072},
	file = {Brown et al. - 2001 - Feature dependence in the automatic identification.pdf:/home/bakunowski/Zotero/storage/HQ45XX59/Brown et al. - 2001 - Feature dependence in the automatic identification.pdf:application/pdf}
}

@article{dahlstedt_creating_nodate,
	title = {Creating and {Exploring} {Huge} {Parameter} {Spaces}: {Interactive} {Evolution} as a {Tool} for {Sound} {Generation}},
	abstract = {In this paper, a program is presented that applies interactive evolution to sound generation, i.e., preferred individuals are repeatedly selected from a population of genetically bred sound objects, created with various synthesis and pattern generation algorithms. This simplifies aural exploration of huge synthesis parameter spaces, and presents a possibility for the sound artist to create new sound engines customized for this kind of creation and exploration – sound engines too complex to control in any other way. Different sound engines are presented, together with a discussion of compositional applications. It is also shown how this technique can be used to simplify sound design in standard hardware synthesizers, a task normally avoided by most musicians, due to the required amount of technical understanding.},
	language = {en},
	author = {Dahlstedt, Palle},
	pages = {8},
	file = {Dahlstedt - Creating and Exploring Huge Parameter Spaces Inte.pdf:/home/bakunowski/Zotero/storage/C7XN2T3E/Dahlstedt - Creating and Exploring Huge Parameter Spaces Inte.pdf:application/pdf}
}